%% Initialization
clear ; close all; clc

sum_accuracy = sum_pr = sum_rec = sum_mcc = 0;
begin = time();

for i = 1:5
	printf("\nTest Run : %d .......... \n",i);
	%% Setting up the parameters.
	input_layer_size  = 4;  % 4 X 1 Input Images of Digits
	hidden_layer_size = 40;   % 10 hidden units
	num_labels = 2;           % 2 labels, from 0 to 1   

	%Load data
	data = load('-ascii' , 'data_banknote_authentication.txt');

	sz = size(data,1);

	%Randomize dataset
	data = data(randperm(sz),:);

	%Create train and test subsets
	train = data(1 : floor(0.7 * sz), :);
	test = data(floor(0.7 * sz) + 1 : end, :);

	%Extract X and y from train dataset
	X_train = train(:,1:4);
	y_train = train(:,5) + ones(size(train,1),1);%Express 0/1 as 1/2
				   %so as to make it work with rest 
				   %of the helper functions.

	X_test = test(:,1:4);
	y_test = test(:,5) + ones(sz - floor(0.7 * sz), 1);

	%Initialize NN parameters 
	initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);
	initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);

	% Unroll parameters
	initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];

	%Backpropagation with Regularization
	lambda = 10;

	%%%Training ANN
	%The second argument is the number of iterations for the minimization routine to run.
	options = optimset('MaxIter', 20);

	%Regularization parameter : lambda
	lambda = 1;

	% Create "short hand" for the cost function to be minimized
	costFunction = @(p) nnCostFunction(p, ...
		                           input_layer_size, ...
		                           hidden_layer_size, ...
		                           num_labels, X_train, y_train, lambda);

	% Now, costFunction is a function that takes in only one argument (the
	% neural network parameters)
	[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);

	% Obtain Theta1 and Theta2 back from nn_params
	Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
		         hidden_layer_size, (input_layer_size + 1));

	Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
		         num_labels, (hidden_layer_size + 1));

	fprintf("Time taken to train : %f \n",time() - begin);

	%Check accuracy
	pred = predict(Theta1, Theta2, X_test);
	acc = mean(double(pred == y_test)) * 100;
	
	tp = fp = tn = fn = 0;

	for j = 1:size(pred,1)
		if(pred(j,1) == 2 && y_test(j,1) == 2)
			tp += 1;
		elseif(pred(j,1) == 2 && y_test(j,1) == 1)
			fp += 1;
		elseif(pred(j,1) == 1 && y_test(j,1) == 1)
			tn += 1;
		else
			fn += 1;	
		endif	
	end

	precision = tp/(tp + fp);
	recall = tp/(tp + fn);
	mcc = (tp * tn - fp * fn)/sqrt( (tp + fp)*(tp + fn)*(tn + fp)*(tn + fn) );	
	
	fprintf('\nPrecision : %f',precision);		
	fprintf('\nRecall : %f',recall);
	fprintf('\nMathew''s Correlation Coefficient : %f',mcc);	
	
	fprintf('\nTest Set Accuracy: %f \n', acc);
	
	sum_accuracy += acc;
	sum_mcc += mcc;
	sum_pr += precision;
	sum_rec += recall;	
end

fprintf("\nAverage accuracy : %f\n",(sum_accuracy/5));
fprintf("\nAverage MCC : %f\n",(sum_mcc/5));
fprintf("\nAverage Precision : %f\n",(sum_pr/5));
fprintf("\nAverage Recall : %f\n",(sum_rec/5));
